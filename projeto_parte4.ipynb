{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79134f4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75566a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports básicos\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# PySpark imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "# Carregar variáveis de ambiente\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Criar SparkSession\n",
    "spark = SparkSession.builder.appName(\"KafkaIntegration\").getOrCreate()\n",
    "base_path = os.getenv('BASE_PATH')\n",
    "\n",
    "# Carregar modelo treinado\n",
    "modelo_carregado = GBTClassificationModel.load(\"modelos/gradient_boosting_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9217dcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enviado: {'id': 2669945285, 'total_transactions': 1523, 'total_spent': 6320.52, 'avg_spent': 4.15, 'first_purchase': '2012-03-03', 'last_purchase': '2013-04-16', 'unique_categories': 248, 'unique_products': 312}\n",
      "Enviado: {'id': 2669952782, 'total_transactions': 345, 'total_spent': 2229.0, 'avg_spent': 6.46, 'first_purchase': '2012-03-12', 'last_purchase': '2013-06-16', 'unique_categories': 108, 'unique_products': 145}\n",
      "Enviado: {'id': 266996275, 'total_transactions': 1258, 'total_spent': 7447.03, 'avg_spent': 5.92, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-02', 'unique_categories': 191, 'unique_products': 212}\n",
      "Enviado: {'id': 2670041982, 'total_transactions': 656, 'total_spent': 5417.1, 'avg_spent': 8.26, 'first_purchase': '2012-03-02', 'last_purchase': '2013-06-18', 'unique_categories': 160, 'unique_products': 279}\n",
      "Enviado: {'id': 267008595, 'total_transactions': 777, 'total_spent': 2728.3, 'avg_spent': 3.51, 'first_purchase': '2012-03-09', 'last_purchase': '2013-06-19', 'unique_categories': 110, 'unique_products': 116}\n",
      "Enviado: {'id': 2670246342, 'total_transactions': 408, 'total_spent': 2088.23, 'avg_spent': 5.12, 'first_purchase': '2012-03-09', 'last_purchase': '2013-04-13', 'unique_categories': 136, 'unique_products': 135}\n",
      "Enviado: {'id': 2670267527, 'total_transactions': 431, 'total_spent': 3430.56, 'avg_spent': 7.96, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-04', 'unique_categories': 141, 'unique_products': 204}\n",
      "Enviado: {'id': 267030737, 'total_transactions': 1066, 'total_spent': 3462.74, 'avg_spent': 3.25, 'first_purchase': '2012-03-04', 'last_purchase': '2013-03-24', 'unique_categories': 182, 'unique_products': 181}\n",
      "Enviado: {'id': 2670316718, 'total_transactions': 1417, 'total_spent': 10597.82, 'avg_spent': 7.48, 'first_purchase': '2012-03-06', 'last_purchase': '2013-06-17', 'unique_categories': 296, 'unique_products': 448}\n",
      "Enviado: {'id': 2670318116, 'total_transactions': 209, 'total_spent': 1654.23, 'avg_spent': 7.91, 'first_purchase': '2012-04-07', 'last_purchase': '2013-03-21', 'unique_categories': 86, 'unique_products': 125}\n"
     ]
    }
   ],
   "source": [
    "# Configurações do Kafka\n",
    "topic = 'customer-data'\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "# Inicializar o produtor Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Carregar dados de clientes\n",
    "base_path = os.getenv('BASE_PATH ')\n",
    "df_customer_features = spark.read.csv(f\"data/customer_features.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_customer_features.write.mode(\"Overwrite\").parquet(\"/home/jovyan/code/data-ml\")\n",
    "df_customer_parquet = spark.read.parquet(\"/home/jovyan/code/data-ml\")\n",
    "# df_customer_features = df_customer_features.limit(10)  # Limitar para exemplo\n",
    "df_customer_features = df_customer_parquet.limit(10)\n",
    "\n",
    "# Enviar dados para o tópico Kafka\n",
    "for row in df_customer_features.collect():\n",
    "    data = row.asDict()\n",
    "\n",
    "    # Corrigir campos que são datas\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, datetime.date):\n",
    "            data[k] = v.isoformat()\n",
    "\n",
    "    producer.send(topic, value=data)\n",
    "    print(f\"Enviado: {data}\")\n",
    "    time.sleep(0.5)  # Simular streaming\n",
    "\n",
    "producer.flush()\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d907103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando consumo das mensagens...\n",
      "Recebido: {'id': 267008595, 'total_transactions': 777, 'total_spent': 2728.3, 'avg_spent': 3.51, 'first_purchase': '2012-03-09', 'last_purchase': '2013-06-19', 'unique_categories': 110, 'unique_products': 116}\n",
      "+---------+----------+--------------------+\n",
      "|       id|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|267008595|       0.0|[0.72774931623968...|\n",
      "+---------+----------+--------------------+\n",
      "\n",
      "Recebido: {'id': 2670267527, 'total_transactions': 431, 'total_spent': 3430.56, 'avg_spent': 7.96, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-04', 'unique_categories': 141, 'unique_products': 204}\n",
      "Erro ao processar mensagem: [VALUE_OUT_OF_BOUND] Value for `obj` must be greater than 2147483647 or less than -2147483648, got 2670267527\n",
      "Mensagem com problema: {'id': 2670267527, 'total_transactions': 431, 'total_spent': 3430.56, 'avg_spent': 7.96, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-04', 'unique_categories': 141, 'unique_products': 204}\n",
      "Recebido: {'id': 2670316718, 'total_transactions': 1417, 'total_spent': 10597.82, 'avg_spent': 7.48, 'first_purchase': '2012-03-06', 'last_purchase': '2013-06-17', 'unique_categories': 296, 'unique_products': 448}\n",
      "Erro ao processar mensagem: [VALUE_OUT_OF_BOUND] Value for `obj` must be greater than 2147483647 or less than -2147483648, got 2670316718\n",
      "Mensagem com problema: {'id': 2670316718, 'total_transactions': 1417, 'total_spent': 10597.82, 'avg_spent': 7.48, 'first_purchase': '2012-03-06', 'last_purchase': '2013-06-17', 'unique_categories': 296, 'unique_products': 448}\n",
      "Recebido: {'id': 2669945285, 'total_transactions': 1523, 'total_spent': 6320.52, 'avg_spent': 4.15, 'first_purchase': '2012-03-03', 'last_purchase': '2013-04-16', 'unique_categories': 248, 'unique_products': 312}\n",
      "Erro ao processar mensagem: [VALUE_OUT_OF_BOUND] Value for `obj` must be greater than 2147483647 or less than -2147483648, got 2669945285\n",
      "Mensagem com problema: {'id': 2669945285, 'total_transactions': 1523, 'total_spent': 6320.52, 'avg_spent': 4.15, 'first_purchase': '2012-03-03', 'last_purchase': '2013-04-16', 'unique_categories': 248, 'unique_products': 312}\n",
      "Recebido: {'id': 2670041982, 'total_transactions': 656, 'total_spent': 5417.1, 'avg_spent': 8.26, 'first_purchase': '2012-03-02', 'last_purchase': '2013-06-18', 'unique_categories': 160, 'unique_products': 279}\n",
      "Erro ao processar mensagem: [VALUE_OUT_OF_BOUND] Value for `obj` must be greater than 2147483647 or less than -2147483648, got 2670041982\n",
      "Mensagem com problema: {'id': 2670041982, 'total_transactions': 656, 'total_spent': 5417.1, 'avg_spent': 8.26, 'first_purchase': '2012-03-02', 'last_purchase': '2013-06-18', 'unique_categories': 160, 'unique_products': 279}\n",
      "Recebido: {'id': 2670246342, 'total_transactions': 408, 'total_spent': 2088.23, 'avg_spent': 5.12, 'first_purchase': '2012-03-09', 'last_purchase': '2013-04-13', 'unique_categories': 136, 'unique_products': 135}\n",
      "Erro ao processar mensagem: [VALUE_OUT_OF_BOUND] Value for `obj` must be greater than 2147483647 or less than -2147483648, got 2670246342\n",
      "Mensagem com problema: {'id': 2670246342, 'total_transactions': 408, 'total_spent': 2088.23, 'avg_spent': 5.12, 'first_purchase': '2012-03-09', 'last_purchase': '2013-04-13', 'unique_categories': 136, 'unique_products': 135}\n",
      "Recebido: {'id': 2669952782, 'total_transactions': 345, 'total_spent': 2229.0, 'avg_spent': 6.46, 'first_purchase': '2012-03-12', 'last_purchase': '2013-06-16', 'unique_categories': 108, 'unique_products': 145}\n",
      "Erro ao processar mensagem: [VALUE_OUT_OF_BOUND] Value for `obj` must be greater than 2147483647 or less than -2147483648, got 2669952782\n",
      "Mensagem com problema: {'id': 2669952782, 'total_transactions': 345, 'total_spent': 2229.0, 'avg_spent': 6.46, 'first_purchase': '2012-03-12', 'last_purchase': '2013-06-16', 'unique_categories': 108, 'unique_products': 145}\n",
      "Recebido: {'id': 266996275, 'total_transactions': 1258, 'total_spent': 7447.03, 'avg_spent': 5.92, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-02', 'unique_categories': 191, 'unique_products': 212}\n",
      "+---------+----------+--------------------+\n",
      "|       id|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|266996275|       0.0|[0.68422999333288...|\n",
      "+---------+----------+--------------------+\n",
      "\n",
      "Recebido: {'id': 267030737, 'total_transactions': 1066, 'total_spent': 3462.74, 'avg_spent': 3.25, 'first_purchase': '2012-03-04', 'last_purchase': '2013-03-24', 'unique_categories': 182, 'unique_products': 181}\n",
      "+---------+----------+--------------------+\n",
      "|       id|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|267030737|       0.0|[0.72737108226805...|\n",
      "+---------+----------+--------------------+\n",
      "\n",
      "Recebido: {'id': 2670318116, 'total_transactions': 209, 'total_spent': 1654.23, 'avg_spent': 7.91, 'first_purchase': '2012-04-07', 'last_purchase': '2013-03-21', 'unique_categories': 86, 'unique_products': 125}\n",
      "Erro ao processar mensagem: [VALUE_OUT_OF_BOUND] Value for `obj` must be greater than 2147483647 or less than -2147483648, got 2670318116\n",
      "Mensagem com problema: {'id': 2670318116, 'total_transactions': 209, 'total_spent': 1654.23, 'avg_spent': 7.91, 'first_purchase': '2012-04-07', 'last_purchase': '2013-03-21', 'unique_categories': 86, 'unique_products': 125}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando consumo das mensagens...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRecebido: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mraw_data\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:1188\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:1160\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1159\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1160\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1162\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1166\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:684\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    682\u001b[0m timer \u001b[38;5;241m=\u001b[39m Timer(timeout_ms)\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 684\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:731\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timer, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    728\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoll: do not have all fetch positions...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    729\u001b[0m     poll_timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(poll_timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 731\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoll_timeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/client_async.py:685\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    678\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    679\u001b[0m             user_timeout_ms,\n\u001b[1;32m    680\u001b[0m             metadata_timeout_ms,\n\u001b[1;32m    681\u001b[0m             idle_connection_timeout_ms,\n\u001b[1;32m    682\u001b[0m             request_timeout_ms)\n\u001b[1;32m    683\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    689\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/client_async.py:728\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    727\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 728\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import json\n",
    "\n",
    "# Esquema esperado para criar o DataFrame Spark\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"total_transactions\", DoubleType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True),\n",
    "    StructField(\"avg_spent\", DoubleType(), True),\n",
    "    StructField(\"first_purchase\", StringType(), True),\n",
    "    StructField(\"last_purchase\", StringType(), True),\n",
    "    StructField(\"unique_categories\", DoubleType(), True),\n",
    "    StructField(\"unique_products\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# VectorAssembler - as mesmas features usadas no treino\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"total_transactions\", \"total_spent\", \"avg_spent\", \"unique_categories\", \"unique_products\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Inicializar consumidor Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    'customer-data',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='spark-consumer-group'\n",
    ")\n",
    "\n",
    "# Funções auxiliares para conversão segura\n",
    "def safe_float(value, default=0.0):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def safe_int(value, default=0):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "print(\"Iniciando consumo das mensagens...\")\n",
    "\n",
    "for message in consumer:\n",
    "    raw_data = message.value\n",
    "    print(f\"Recebido: {raw_data}\")\n",
    "\n",
    "    try:\n",
    "        # Garantir tipos corretos e preencher valores default\n",
    "        data = {\n",
    "            \"id\": safe_int(raw_data.get(\"id\")),\n",
    "            \"total_transactions\": safe_float(raw_data.get(\"total_transactions\")),\n",
    "            \"total_spent\": safe_float(raw_data.get(\"total_spent\")),\n",
    "            \"avg_spent\": safe_float(raw_data.get(\"avg_spent\")),\n",
    "            \"first_purchase\": raw_data.get(\"first_purchase\", \"\"),\n",
    "            \"last_purchase\": raw_data.get(\"last_purchase\", \"\"),\n",
    "            \"unique_categories\": safe_float(raw_data.get(\"unique_categories\")),\n",
    "            \"unique_products\": safe_float(raw_data.get(\"unique_products\"))\n",
    "        }\n",
    "\n",
    "        # Criar DataFrame Spark com o esquema definido\n",
    "        df = spark.createDataFrame([Row(**data)], schema=schema)\n",
    "\n",
    "        # Criar vetor features para o modelo\n",
    "        df_features = assembler.transform(df)\n",
    "\n",
    "        # Aplicar o modelo carregado (GBTClassificationModel)\n",
    "        pred = modelo_carregado.transform(df_features)\n",
    "\n",
    "        # Mostrar a predição\n",
    "        pred.select(\"id\", \"prediction\", \"probability\").show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar mensagem: {e}\")\n",
    "        print(f\"Mensagem com problema: {raw_data}\")\n",
    "        continue\n",
    "\n",
    "    # Se quiser testar só 1 mensagem, descomente o break abaixo\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b3d1c",
   "metadata": {},
   "source": [
    "# KAFKA - Classicação dos dados dos clientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbb051",
   "metadata": {},
   "source": [
    "**Configuração e carregamento do modelo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02578faf",
   "metadata": {},
   "source": [
    "Inicialização das bibliotecas necessárias, criação da sessão Spark e carregamento do modelo Gradient Boosting pré-treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae82b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from modelos/gradient_boosting_model\n"
     ]
    }
   ],
   "source": [
    "# Imports básicos\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "# Carregar variáveis de ambiente\n",
    "load_dotenv('/home/jovyan/code/.env')\n",
    "\n",
    "# Criação da sessão Spark\n",
    "spark = SparkSession.builder.appName(\"KafkaIntegration\").getOrCreate()\n",
    "base_path = os.getenv('BASE_PATH', '/home/jovyan/code/data')\n",
    "\n",
    "# Caminho do modelo\n",
    "model_path = \"modelos/gradient_boosting_model\"\n",
    "if not os.path.exists(f\"{model_path}/metadata\"):\n",
    "    raise Exception(f\"Model not found at {model_path}/metadata\")\n",
    "\n",
    "# Carregar modelo já treinado\n",
    "modelo_carregado = GBTClassificationModel.load(model_path)\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece255b",
   "metadata": {},
   "source": [
    "**Kafka Producer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bdc93",
   "metadata": {},
   "source": [
    "Configuração do producer Kafka, para enviar os dados do cliente para o tópico de dados do cliente. Realiza a leitura do customer_features.csv, guarda-o como parquet, maximizando a eficiência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4d1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enviado: {'id': 2669945285, 'total_transactions': 1523, 'total_spent': 6320.52, 'avg_spent': 4.15, 'first_purchase': '2012-03-03', 'last_purchase': '2013-04-16', 'unique_categories': 248, 'unique_products': 312}\n",
      "Enviado: {'id': 2669952782, 'total_transactions': 345, 'total_spent': 2229.0, 'avg_spent': 6.46, 'first_purchase': '2012-03-12', 'last_purchase': '2013-06-16', 'unique_categories': 108, 'unique_products': 145}\n",
      "Enviado: {'id': 266996275, 'total_transactions': 1258, 'total_spent': 7447.03, 'avg_spent': 5.92, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-02', 'unique_categories': 191, 'unique_products': 212}\n",
      "Enviado: {'id': 2670041982, 'total_transactions': 656, 'total_spent': 5417.1, 'avg_spent': 8.26, 'first_purchase': '2012-03-02', 'last_purchase': '2013-06-18', 'unique_categories': 160, 'unique_products': 279}\n",
      "Enviado: {'id': 267008595, 'total_transactions': 777, 'total_spent': 2728.3, 'avg_spent': 3.51, 'first_purchase': '2012-03-09', 'last_purchase': '2013-06-19', 'unique_categories': 110, 'unique_products': 116}\n",
      "Enviado: {'id': 2670246342, 'total_transactions': 408, 'total_spent': 2088.23, 'avg_spent': 5.12, 'first_purchase': '2012-03-09', 'last_purchase': '2013-04-13', 'unique_categories': 136, 'unique_products': 135}\n",
      "Enviado: {'id': 2670267527, 'total_transactions': 431, 'total_spent': 3430.56, 'avg_spent': 7.96, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-04', 'unique_categories': 141, 'unique_products': 204}\n",
      "Enviado: {'id': 267030737, 'total_transactions': 1066, 'total_spent': 3462.74, 'avg_spent': 3.25, 'first_purchase': '2012-03-04', 'last_purchase': '2013-03-24', 'unique_categories': 182, 'unique_products': 181}\n",
      "Enviado: {'id': 2670316718, 'total_transactions': 1417, 'total_spent': 10597.82, 'avg_spent': 7.48, 'first_purchase': '2012-03-06', 'last_purchase': '2013-06-17', 'unique_categories': 296, 'unique_products': 448}\n",
      "Enviado: {'id': 2670318116, 'total_transactions': 209, 'total_spent': 1654.23, 'avg_spent': 7.91, 'first_purchase': '2012-04-07', 'last_purchase': '2013-03-21', 'unique_categories': 86, 'unique_products': 125}\n"
     ]
    }
   ],
   "source": [
    "# Configurações do Kafka\n",
    "topic = 'customer-data'\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "# Iniciar o produtor Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Carregar dados de clientes\n",
    "df_customer_features = spark.read.csv(f\"{base_path}/customer_features.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Salvar como Parquet\n",
    "df_customer_features.write.mode(\"overwrite\").parquet(f\"{base_path}-ml/customer_features.parquet\")\n",
    "df_customer_parquet = spark.read.parquet(f\"{base_path}-ml/customer_features.parquet\")\n",
    "df_customer_features = df_customer_parquet.limit(10)  # Limitar para exemplo\n",
    "\n",
    "# Enviar dados para o tópico Kafka\n",
    "for row in df_customer_features.collect():\n",
    "    data = row.asDict()\n",
    "\n",
    "    # Corrigir campos que são datas\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, datetime.date):\n",
    "            data[k] = v.isoformat()\n",
    "\n",
    "    producer.send(topic, value=data)\n",
    "    print(f\"Enviado: {data}\")\n",
    "    time.sleep(0.5)  # Simular streaming\n",
    "\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66cdbe",
   "metadata": {},
   "source": [
    "**Consumidor Kafka (Processamento de Mensagem Única)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632b1df",
   "metadata": {},
   "source": [
    "Configuração de um consumer kafka, de forma a ler as mensagens do tópico de dados do cliente, uma de cada vez. Define um schema para os dados recebidos, cria um DataFrame para cada mensagem e utiliza o modelo carregado para prever a classificação do cliente. Produz previsões para cada mensagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando consumo das mensagens...\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import json\n",
    "\n",
    "# Schema esperado para criar o DataFrame Spark\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"total_transactions\", DoubleType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True),\n",
    "    StructField(\"avg_spent\", DoubleType(), True),\n",
    "    StructField(\"first_purchase\", StringType(), True),\n",
    "    StructField(\"last_purchase\", StringType(), True),\n",
    "    StructField(\"unique_categories\", DoubleType(), True),\n",
    "    StructField(\"unique_products\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# VectorAssembler - usa as mesmas features usadas no treino\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"total_transactions\", \"total_spent\", \"avg_spent\", \"unique_categories\", \"unique_products\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Iniciar consumer Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    'customer-data',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='spark-consumer-group'\n",
    ")\n",
    "\n",
    "# Funções auxiliares para conversão\n",
    "def safe_float(value, default=0.0):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def safe_long(value, default=0):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "print(\"Iniciando consumo das mensagens...\")\n",
    "\n",
    "for message in consumer:\n",
    "    raw_data = message.value\n",
    "    print(f\"Recebido: {raw_data}\")\n",
    "\n",
    "    try:\n",
    "        # Garantir tipos corretos\n",
    "        data = {\n",
    "            \"id\": safe_long(raw_data.get(\"id\")),\n",
    "            \"total_transactions\": safe_float(raw_data.get(\"total_transactions\")),\n",
    "            \"total_spent\": safe_float(raw_data.get(\"total_spent\")),\n",
    "            \"avg_spent\": safe_float(raw_data.get(\"avg_spent\")),\n",
    "            \"first_purchase\": raw_data.get(\"first_purchase\", \"\"),\n",
    "            \"last_purchase\": raw_data.get(\"last_purchase\", \"\"),\n",
    "            \"unique_categories\": safe_float(raw_data.get(\"unique_categories\")),\n",
    "            \"unique_products\": safe_float(raw_data.get(\"unique_products\"))\n",
    "        }\n",
    "\n",
    "        # Criar DataFrame Spark com o esquema definido\n",
    "        df = spark.createDataFrame([Row(**data)], schema=schema)\n",
    "\n",
    "        # Criar vetor features para o modelo\n",
    "        df_features = assembler.transform(df)\n",
    "\n",
    "        # Aplicar o modelo carregado -> GBTClassificationModel\n",
    "        pred = modelo_carregado.transform(df_features)\n",
    "\n",
    "        # Mostrar a previsão\n",
    "        pred.select(\"id\", \"prediction\", \"probability\").show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar mensagem: {e}\")\n",
    "        print(f\"Mensagem com problema: {raw_data}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf69b5",
   "metadata": {},
   "source": [
    "**Consumidor Kafka (Processamento em Batch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80a0dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando consumo das mensagens...\n",
      "Recebido: {'id': 2669945285, 'total_transactions': 1523, 'total_spent': 6320.52, 'avg_spent': 4.15, 'first_purchase': '2012-03-03', 'last_purchase': '2013-04-16', 'unique_categories': 248, 'unique_products': 312}\n",
      "Recebido: {'id': 2669952782, 'total_transactions': 345, 'total_spent': 2229.0, 'avg_spent': 6.46, 'first_purchase': '2012-03-12', 'last_purchase': '2013-06-16', 'unique_categories': 108, 'unique_products': 145}\n",
      "Recebido: {'id': 266996275, 'total_transactions': 1258, 'total_spent': 7447.03, 'avg_spent': 5.92, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-02', 'unique_categories': 191, 'unique_products': 212}\n",
      "Recebido: {'id': 2670041982, 'total_transactions': 656, 'total_spent': 5417.1, 'avg_spent': 8.26, 'first_purchase': '2012-03-02', 'last_purchase': '2013-06-18', 'unique_categories': 160, 'unique_products': 279}\n",
      "Recebido: {'id': 267008595, 'total_transactions': 777, 'total_spent': 2728.3, 'avg_spent': 3.51, 'first_purchase': '2012-03-09', 'last_purchase': '2013-06-19', 'unique_categories': 110, 'unique_products': 116}\n",
      "Recebido: {'id': 267030737, 'total_transactions': 1066, 'total_spent': 3462.74, 'avg_spent': 3.25, 'first_purchase': '2012-03-04', 'last_purchase': '2013-03-24', 'unique_categories': 182, 'unique_products': 181}\n",
      "Recebido: {'id': 2670316718, 'total_transactions': 1417, 'total_spent': 10597.82, 'avg_spent': 7.48, 'first_purchase': '2012-03-06', 'last_purchase': '2013-06-17', 'unique_categories': 296, 'unique_products': 448}\n",
      "Recebido: {'id': 2670267527, 'total_transactions': 431, 'total_spent': 3430.56, 'avg_spent': 7.96, 'first_purchase': '2012-03-02', 'last_purchase': '2013-04-04', 'unique_categories': 141, 'unique_products': 204}\n",
      "Recebido: {'id': 2670246342, 'total_transactions': 408, 'total_spent': 2088.23, 'avg_spent': 5.12, 'first_purchase': '2012-03-09', 'last_purchase': '2013-04-13', 'unique_categories': 136, 'unique_products': 135}\n",
      "Recebido: {'id': 2670318116, 'total_transactions': 209, 'total_spent': 1654.23, 'avg_spent': 7.91, 'first_purchase': '2012-04-07', 'last_purchase': '2013-03-21', 'unique_categories': 86, 'unique_products': 125}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m message_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando consumo das mensagens...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRecebido: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mraw_data\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:1188\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:1160\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1159\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1160\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1162\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1166\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:684\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    682\u001b[0m timer \u001b[38;5;241m=\u001b[39m Timer(timeout_ms)\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 684\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/consumer/group.py:731\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timer, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    728\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoll: do not have all fetch positions...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    729\u001b[0m     poll_timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(poll_timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 731\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoll_timeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/client_async.py:685\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    678\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    679\u001b[0m             user_timeout_ms,\n\u001b[1;32m    680\u001b[0m             metadata_timeout_ms,\n\u001b[1;32m    681\u001b[0m             idle_connection_timeout_ms,\n\u001b[1;32m    682\u001b[0m             request_timeout_ms)\n\u001b[1;32m    683\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    689\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/kafka/client_async.py:728\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    727\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 728\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import json\n",
    "\n",
    "# Esquema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"total_transactions\", DoubleType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True),\n",
    "    StructField(\"avg_spent\", DoubleType(), True),\n",
    "    StructField(\"first_purchase\", StringType(), True),\n",
    "    StructField(\"last_purchase\", StringType(), True),\n",
    "    StructField(\"unique_categories\", DoubleType(), True),\n",
    "    StructField(\"unique_products\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"total_transactions\", \"total_spent\", \"avg_spent\", \"unique_categories\", \"unique_products\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Consumidor Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    'customer-data',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='spark-consumer-group'\n",
    ")\n",
    "\n",
    "# Funções auxiliares\n",
    "def safe_float(value, default=0.0):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def safe_long(value, default=0):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "# Processar em lotes\n",
    "batch_size = 100  # Process 100 messages at a time\n",
    "max_messages = 1000  # Stop after 1000 messages\n",
    "batch_data = []\n",
    "message_count = 0\n",
    "\n",
    "print(\"Iniciando consumo das mensagens...\")\n",
    "\n",
    "for message in consumer:\n",
    "    raw_data = message.value\n",
    "    print(f\"Recebido: {raw_data}\")\n",
    "\n",
    "    try:\n",
    "        data = {\n",
    "            \"id\": safe_long(raw_data.get(\"id\")),\n",
    "            \"total_transactions\": safe_float(raw_data.get(\"total_transactions\")),\n",
    "            \"total_spent\": safe_float(raw_data.get(\"total_spent\")),\n",
    "            \"avg_spent\": safe_float(raw_data.get(\"avg_spent\")),\n",
    "            \"first_purchase\": raw_data.get(\"first_purchase\", \"\"),\n",
    "            \"last_purchase\": raw_data.get(\"last_purchase\", \"\"),\n",
    "            \"unique_categories\": safe_float(raw_data.get(\"unique_categories\")),\n",
    "            \"unique_products\": safe_float(raw_data.get(\"unique_products\"))\n",
    "        }\n",
    "        batch_data.append(Row(**data))\n",
    "        message_count += 1\n",
    "\n",
    "        # Processar lote quando atingir batch_size ou max_messages\n",
    "        if len(batch_data) >= batch_size or message_count >= max_messages:\n",
    "            df = spark.createDataFrame(batch_data, schema=schema)\n",
    "            df_features = assembler.transform(df)\n",
    "            pred = modelo_carregado.transform(df_features)\n",
    "            pred.select(\"id\", \"prediction\", \"probability\").show()\n",
    "            # Salvar lote em CSV\n",
    "            pred.select(\"id\", \"prediction\", \"probability\").write.mode(\"append\").csv(\n",
    "                f\"{base_path}-ml/submission_batch\", header=True\n",
    "            )\n",
    "            batch_data = []  # Limpar lote\n",
    "\n",
    "        if message_count >= max_messages:\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar mensagem: {e}\")\n",
    "        print(f\"Mensagem com problema: {raw_data}\")\n",
    "        continue\n",
    "\n",
    "# Processar quaisquer mensagens restantes\n",
    "if batch_data:\n",
    "    df = spark.createDataFrame(batch_data, schema=schema)\n",
    "    df_features = assembler.transform(df)\n",
    "    pred = modelo_carregado.transform(df_features)\n",
    "    pred.select(\"id\", \"prediction\", \"probability\").show()\n",
    "    pred.select(\"id\", \"prediction\", \"probability\").write.mode(\"append\").csv(\n",
    "        f\"{base_path}-ml/submission_batch\", header=True\n",
    "    )\n",
    "\n",
    "consumer.close()\n",
    "print(\"Consumer finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
