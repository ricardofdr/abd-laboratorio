{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79134f4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75566a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa366aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataPreparation\").getOrCreate()\n",
    "base_path = os.getenv('BASE_PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d65822",
   "metadata": {},
   "source": [
    "# 2. DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbfbb391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�}ES\u0002\u0003offers.csv\u0000���n�0\f\u0005��=\u000bQ�����u�0`K� \u001d�������\u0002��Y\u0012)�^__��|��|?�����p���\\������x���=�|{������\u0017�fW�\u0016q\u0012\u0012.g�brjm�\u0015\u0018�Q7�\u0000)��u\u001d�X�t\u0006)XG\u001e&fa�O�$j9�(���׶\u0007#A��䜘\u0002�Bw��\n",
      ",\u001a\u001ed�2�\u000b3;�b4[&�\u0007\u0000̞E��\u001ff�l<�Ȉ���D�\"\u0010\u001f\u0004�\u0011�\u0013#u�\u0007��V҃��Y�@���ʎ��W;XKU�C�6�\u0000��\u001c�5j�T���g��̛�ܮA�U=@AV0',%�W\u0011��M�=Sp!��^�����9�{�h�Ȭ��o�F#F�|l�W�Yխ�X���\u001d#\n",
      "�H�l*j#����A�ø�8f\u0010\u0005���Br��Eh�\u000f%C�-\u000f��r��\u000e�\t�\u0015�+���+C���\tRL�5B�\u0001�\u0007�r�V\u0015\u0005\u0000\u0000head: cannot open 'data/sampleSubmission.csv.gz' for reading: No such file or directory\n",
      "�.��\u0002\u0003testHistory.csv\u0000|�[�$5\f��YK!�r�c/\u0007q\u0001\u0012b�\"\u001e��\u000b�G�4��N������\u000f��?����\u001f?����o������\u001f����o�,M\u0012Wǥ����e�����!H��C�fԃ�C��������\t�C�!;/��\"\u000fxk�oԅH�K�Z�n@\u000f�E�.�\u001d D�\u0000#\u0017��k��V}�QT�4^@߀\n",
      "\u0019�+`��:\u0016�\u0007 \u0010\"׽E����B,��V��\n",
      "|\u00004���\u0014��\tPQ%�����\u0001�\u0015��^�ߧ��]Z�s��Kc\u0002h�/Ľě�\u001e�E\u001c_�p1���VG���Y9^\u001a�ᅐ\u0010�\u001b�z\u0007|\u0000��}\u0001�%\\�NJ�\u0000�\u0010��(}�1T�=�\u0001�t��\u0001\u001b_\\\u0019�p�K�J\u000b��8\u0000�|~��\u001ax�K\n",
      "�\u000b�\u0013\u001a��i�@\u001f�T\u000b���مC���5PR�%\t�\u00004���^9DzF|�n�ЦQk��\u0010\u0010�ȯ\u001d\u001aԤ]���]L���\u0013�+���#\u001bQ^\u0019��TT�E�e����1����(���n�G:�Rp�u�8�IE�\u0012X~�0�����5��,���'䪷��#Ǚa\u001a���\u0002�ն��Gζ*Y�\u000f\u0007��\u0014|\f�ɾ\u0012m+���C{\u0014�\u0012��R���A!c- E\u0005Ċ\u0003T�4��;��(��\u00154}��ߖ�n�\"Ip9F;����R�u\u0000\u0007�?:I9�̞@��\u0017<;^[��V�0\u0000\u0019�\u0014s?rG�b-��\u0014|I��\u0002�\u0017\u0018 ����\u000eO9���\u000fRF�$��\u0006���Hi�3�ߧ\u0002c�����=�L�R��`�2����\u0010�j}\u0007�3aLa��ְ��\u0003TK�\u0012�r�lq���0; �RK�\u001f��\u0018[��\u0002Н\\4�\u001b3���-'�\u0010�.p��!�m���ҁ'a�^���/�+C���<��nsK���\u0011�@\u001f���EํHI�6��\u0010E8���!\u0012���[7�G�\u0018=���\u0010����s\u001d\u0002!^��CK�ƖdV%~]T]z�E\u000e�\n",
      "����0$�m4\u0007�j�-�q�@�l��\u001c =�y�K��!����\u0017~�\u0006�\u0000Tq�I�T�ݛ!\")jۑ}!Bh�F�\u0010���\u0011�C�\n",
      "b�d�.W�:\u0000+Y�\u0015o��\u0012��\u001bRV��7�\u0012N��6\u001f�V�-RC���f�\u0001�B-��\u0001��{\u001boBA�ccF\t��\u0000��Ϡ\u0001�\u0010�\fHǗ&\u001a\u0011�Ō\u001dBՍ�h�\u0003��c\u0019�e�E~��k\t>̴(h��\u001fl�\u0016�Xf�QA�\t���\u0000\u001dV�F�5l\n",
      "\t�m/�v\u0014��.P\u0015�\\\u0003\\\u0006�B�ҎM\u000f�&%���p\u000b&wG\fQR��x��C��=y<���B3*�+�\u001b1�А-��\u0012^l\n",
      "שa�\n",
      "�~��a�ݞ\\��\u0002\"J�s�\u001eB��V�|�R�=���\u0007\u0003\"ck.8�S��=\u001b�\u0010e��qa�N@6bt����5\u001e\u0003g\u000f��e�\u001be?�T��\u00127\u001c�\u0011Q��1Du�>�\u0006(Y��\u0019\u000fQ�jK��\u0001���rf:@Q���\u001c�4s\u001f�#$ �\u001b�C�!��}�\u000b��[9�>�\u000b�t}\u0007\u001d�(�t�\u001c@���%C�ڰ܎9�\u000ek�|��#O��mO�\u0011\u0014���~�*���=������\u001f��\u0012{=�Ri믴�hQ��*��Wn\u001byB���<r~����4���7�G\u0003*%���2�.]�\u001f SMz�0K��J|ރ�\u0000�br�����V��w�\u0018�(�\u001f��l��]�����\u001c\u0010�n%\u0015D-�15r���;GвŗG\u0004�\u0001h\u001e[ۈ\u0003�FJ/�\u000e\u0000I���Te��&�]�!`\u0019�細#\u000f�N,S�\u000fp䔏�_��\u0011n��g.\u001f9�­x�\u0010�4�J9g��}kn��\u001c�}���$���}�)!}\n",
      ":��U�u@\u000f��.z��\u0017��$ʜ��8��x�^�*\f�\\J1\u0007�r�4\fP\n",
      "Y��`�h�\u00022^��\u0004��+�\u0014�쓎\u0019�]��|�2s���\u000e�\tZ�+���~M\u0015�s\u0003S^L� dH��N{���\u0002XT��b�p\u0001�g�M\u000e���F��\u0012�:U&��1ǐ�Ր3�Ҕ�\tVN=�\u0001�o&BQ��QuA��MMq��\u0010�}7���C��%��E���dA�l�������[6\u0014Tc�eR�\u0014ܱ(\u0003S�-Ov(\t��Rɒ�[��\u0011��37\u0005V��T�P.�8*����I%��7�fM5\u0016\u0014�V�0�-sWɛI������\u0000�\u0018����\u0018U\u0015塧>M]>�\\�\u0016��\u001c[��\f��]�S�bw�29�'��(Ƣ�<����R+�\u000e�\u0013�\n",
      "\u0011�i�V^TQ�\u000fJI3��5�SC(�!���L\u0000�\u0016\u0002;�ŰH���hS�8��O�F�qe�i\u0014ȗ��BtX,���B׸3P�v�8J��3Ӧ��\n",
      "����\u0002U�Oc:^�D\"4Y�\u001b\u0012�[-��\u000eWՔ�\u0011�m��P�\u0001�H�L\u0016�\u000f\\�����8!io�����]Ɖ�Մ�ƺ�\n",
      "�7��j�|��}Q�V9�p4\u0015�*srf�T���]�bJsuv\u0017� ���M9�m^P�\t�wC]L��֦��x��\\�6�\u000b\u0016��\u0013�M�!�hޔ\"4�2��[0%U\u0010\u0006��\u0016�*��\n",
      "�-���\u0001\u001b��\"Ru�wo�V8����*n�9\u0010��b�\\\\�R�[��ޘjLBsGKu�H\u001d\u001aC�/(����C��dD��\u000ff�w�+ti\u000b:%�]�\u000f{/�Ϝ�\u001dJ5c\u001e�����k�(�[}jQ %�۳�\u0016*\u0010Հ�cF@��y�\u0014D\u0004���t�/g��\u0004�Qu��n�\u001cEb�yh\u000b��\u0012\u0013��$\u000e��0�ւ��K�u��\u0005)I\tO�]C,�q\u0006N2bļR,*D�|��\u001b��֜\u000b��j�����X��x�����\u0002c�HN��\n",
      "�4��&K\u001f\u0014:�k*c;(��ԟ\u000f=��i��p7�\u001cl�xW�\u0014�05\u001b�\u000b ��w�q\u001b̴u��\u000fîu\u0019\u000ep\u0014�q�����G�u�{������\u0011�,o�5��Q��9*mS�K�;�\u00199!�y�\u0007c\u0017���\u001d\u0004�ko�]�)��(.�!&\"�\u0001\u001a��X\u0001�{���y��,��;������w(Ӳ!\u0015ѣе�����`!v΃�b��漟��6��4�<�(N?u��7\u0007k�Z\u001b\u0003hᵩ=\u000b=�Cy�\u000e��0�[(�n.&�yP��v>\u0007�W�6\u0007�\u0006�۱�d�\u0011ҳ\u001e�\u001a��\u0017Z���\\�V��V��AMz�Ҍ�a@K�<��W�}�����+����l���<�fd�\u0016�1U��1�\u0006,���8\u0003�i�P\u0005�����X��·�\u0019�\u0014��c�\f���_����%�Q3aiO���ƮlA�I��U���r\fCn��\u0003w'\u0019Κ����N\u0014\u001a�o\u0000cs���\u0013r\u0006�*�W�=e{��\n",
      "�\u0002꠰��\u0007\f\u001e\u0003�h��6#A�����H\u001d��\t[�ր\u001d��T2�\u001c��55������wZ��\u000em8*��\u001a4�~r�&�p��1�\n",
      "9e�m�Ku\u0010�����65�e�p\f+�j���&�\tb\u0007\u001f�ZZ��8��n��\u001e�\u0002Gu��zF0Gaˠ-a�P8�\u0018�T�x\u0010��'�\tx�N\u0000�n����.S�_?(:7�e\u0019\u0016�u�:�ۧD��_�浵���AiS�\u0010�i����k2\u00071զ�\u000e��m�\u0013�A�&dۈ��\u0005%v\u001f��V���Q��ߏk�\u000e\u001c\u0016\u0007���2��ٵr5�q\u0010fPG��Cm�ǖL�\u000e�a�\u000e?( �\n",
      "vLV��>�\u001c��9�I�\u00135�ٰYQ��A\u0018�x��0�\u0011�\u0018��䈫tP:�JM��V�V�\u0015�Q\u0005\u000fY�\u001e4���ߣ\u00109+��Al|�Q�z��=�\\�'\u001c\u0014�&_F\u001fT��3�4\u0002��� 2G��\u000e��*)�\u001e�-\u0013��1[h�7�A� ��~\u0007�O�a��\u0005�yA�y\u0004c�����A�Y#���\u0010\u001c\u0013a\n",
      "���!�[���a�\u0017%:�\u0006�XTY*�Y�/\u0003A���m.�v�G\u0016��U��H���\u001a�LZܾ�����b0$.�gIED:�I�Y\u001c���~QO'��X�n�|c�eᑼ�kUj\u0018kѺȑF\u0002&������Am�;N�<Of�Ud��E9�C*r��h���\u001a\u0017sk\u001e��{f\u0019���AcV|�\u0007\u000fUڷ{\u0011|��n�L\u0005�MmU��~�\u0004�=�\u000f\u0014���C����Rsk��IDS\u0016Y�����\"���R�J�=kQw\u0019Od�\u001aՉ�� ES�UD�Q�a�^X��E��W����S\u0017y�)mOZ\u0004�&\u001d�k\"��O�oƛ���J�0�龨M�xR\t926���GE��%J�BY�٢���-\u0007E6_.,*s���E%t��j\u0017�{���-g�v\u0019���>*\u001b��+�\u0000�X~�����M�K<�F�)�l2ّ�*H�s����0�j���\"\n",
      "b\u0018Pv\u0000x|%&�#�\u0016S�V�z�~��zRhܖ�����\u0007��׻�����\th7���\u0007�Γ�<dd�.\u0015I\u0017B�8\u001f\u0015\u0001�G��@}�<�(GH\\�,ǓR�t�>h��:\u0000��u\u0016�w��H8`�\u0014�2>A�ar��t7=\u000f\u0012\u00190\u000b\u0014BË�Bw`�i\u000b��\t\u000e��\u0014D\u0006��� kT���B�>� ��U\u0004\u0000\u001dX\u000f��p�\u001d\u0019W�\f\u001c�L�`hd����8�h`�\u0000 \u0006ώ����N\u0016�`j\u0016��\u0006\u000f���J\u0019�vh�֣��M�<\u0000\u0005\u0001\u0010�\u0001���ĥ�fWe&>�j��]�A�V\u0005���\\ڍ��s�\u0019�\n",
      "a��p�4Ьge�UѪ�Q�\u001d�}\u0010�*�e��\u0018D2�a.\u001az���\tɍN6t\"]1UL�\n",
      "Q\u0000�0CGM\f�\u0016mrL��z/X��˜%��\u0018�L\u0006M�\u0005�A�%г��,$J��4h�\u0004\u0001q�PH��22�b����\u001bG�1\f���p��\u001eq�Q�\u0017WPH\u000e6�)�\u001c�\t�2F�~\u0012\u0010s\u0018&\u001e\u0004��k�EM�N� �a\u0006\u000fJ}[7�9R\u001fD�\t�\u0003����&�Wu>\u0018:\u001f\f4�\u0011�i!){\u000e㠩\u001f�\u001b�\u001d{�O�t\u0013����t\u0001��)Ԝ�\n",
      "8�?&\u0003vC�\tA�\\,ʅ(Q��8��� �y�aʩ\u0004�{)�\u0017$�@J���1��\u0005�t?:\u001e~��'$==��r1)W5Ǒ��ȹ�������� �*\u0012\f�.Cy�z�\u0014f���BP���[�v�\u0017!��\u0003\u0002����i9f�� ���e9�+�\u0011��\f\u0006a��;��o��\u000f3��GWo*\u001b�|\u0010�\u0003ȸ��\t��\u0007�\n",
      "&��V\u0006���70+E�;e��5�\"2M�r\u001dg+����\u001fk\u0006_�\u0017)$ \u001d{E��x\u0015�\u0010�\u000f���d�T�T�\u0011Dc�+��xΎ��\u0011\u0002�Q>��E<�o���< \tw?\u000b)��\u001b0�\u0019�4)�on�GI?y�T��� �7�\u001f}0\u0018F)��^\tH�4�̹$��*��O�zq�Kԥ_l�\u001b,����1\u001d.V�%�tt�~v��\u0016v\u0019���n^\u001e%u]e\u000b#�\\+\fyG\"-�p�8\u0016<�\u001c,3���#M�i\u001aڠu�\t�\u0007�<�Ի�8��}v\u0004\u0010�qX\"AH�\u0019[�Bn�o\u0006�\u001dI@-��fS:\fZ\u000e\u000b��<p����.�ubj\\/t�Ev$J��B�p[)\u000e/[���y��rwi��3�<�Ҧ��\u0016\u0001Ɣ\u0015@�u�5�JV�R���a�\u0017\u001c\u0003�+l,/���\u001f�r�����\u0015�5:\u000b��3a�\\/� ཥ\u0011\u0004͟�z��g^�}��KB/�b��dƮ�\u001fxXb��\u0018n�\u0006Mo`��(8\u0001��9��ASJA3\u0011�\u001c��s>�Rߢ�\u000eHO�Z�\u001a�T\n",
      "�#�g�/=�~+O�\u001e�*B`������\u001f�C�\u001a�\u00013��q����\u000f����&�(7\u0005T�����'��C�C\u0019\u0004����\u0010@�k��e\u0015��ϔ\t#\u001b%[��\u001a\u0019�l\u000b�\u0013�I72h\"1��Q�z>g�-٪Y�\u0013k����r�Nَ`[&ڍʣS��\u0005�cg�Ð�]��<\u0010\u001a\u001e� \fZ��\u0000���B\u0000��a�#��#6=�\u001f��oy�B>isKeo�\u0010�\u001e:�H4�rT���YF�>գ��9�Qz�3�x��\n",
      "4�\u0016P�\n",
      "\u0013��i��7sR������Y��Y�q�\u0013��#�Ih%\u0005Ԡ��}�L��^�\"�M�����#�\u001aL�$\u0006Mw�S��0�+\u0007*\u000b��w�v�ښ^��\u0003�zPnx^ءN�$�\t��i��\"Q��\u0012&:\u00019\u0001��{��=��-���\u0018#�л�\u0019���>h�-\u0010_\u001d\u0015�\u0004�2_=&\u0007!��\u000e]�)3�^'�c�z�����9o^���5�\fhG��c�^��f���qC�\u001bm���׎/ɪ��a����Ø�V<HH�g�2nsdb��!Sbi���J�u��<��8�\u001et�\u001a=�R\u0010\u00039%9�j��j�9=���wčgs�3�(�\u0019��\u0005�Z\n",
      "\u0006ݑ��RO\u001a�_\u0012A�S��#���r��qOH�_�\u001a�t�^\u0010���\u001e\u0019H�.�$[����\u001d��s&�U��n�N\u0006�Y��\u0007/��\u0005u�AP\u001dd��&e���F\u001e�jo#�~Դ�\u001c�իm�\u001d��z]Ku\u0013@q�\u00182OZ@�\u001d��<p�ke�\u001b?n�[\\52�>k\u0005>H�T�\u001bPO����\u001a�?m^_�{J`a}C�E��\u000b�;���\u000by����o�[۵�Â�\u0001B\u0007I`��vڢÍ�h���ɬ����O�󽮅w\u0018ɱ\u0016\u0016\u001e�ʃ�\u001f3.\u00194y�\n",
      "9���MN<��\n"
     ]
    }
   ],
   "source": [
    "# Data to read - offers.csv\n",
    "data_dir_offers = f'{base_path}/offers.csv.gz'\n",
    "data_file_offers = data_dir_offers\n",
    "\n",
    "! head $data_file_offers\n",
    "\n",
    "# Data to read - sampleSubmission.csv\n",
    "data_dir_sampleSubmission = f'{base_path}/sampleSubmission.csv.gz'\n",
    "data_file_sampleSubmission = data_dir_sampleSubmission\n",
    "\n",
    "! head $data_file_sampleSubmission\n",
    "\n",
    "# Data to read - testHistory.csv\n",
    "data_dir_testHistory = f'{base_path}/testHistory.csv.gz'\n",
    "data_file_testHistory = data_dir_testHistory\n",
    "\n",
    "! head $data_file_testHistory\n",
    "\n",
    "# Data to read - trainHistory.csv\n",
    "data_dir_trainHistory = f'{base_path}/trainHistory.csv.gz'\n",
    "data_file_trainHistory = data_dir_trainHistory\n",
    "\n",
    "! head $data_file_trainHistory\n",
    "\n",
    "# Data to read - transactions.csv\n",
    "data_dir_transactions = f'{base_path}/transactions.csv.gz'\n",
    "data_file_transactions = data_dir_transactions\n",
    "\n",
    "! head $data_file_transactions;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c71e15",
   "metadata": {},
   "source": [
    "**Offers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf8bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data - offers.csv\n",
    "df_offers = spark.read.csv(\n",
    "        data_file_offers, \n",
    "        header=True, sep=',', inferSchema=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0f740",
   "metadata": {},
   "source": [
    "**TESTHISTORY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb56c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data - testHistory.csv\n",
    "df_testHistory = spark.read.csv(\n",
    "        data_file_testHistory, \n",
    "        header=True, sep=',', inferSchema=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9dd00",
   "metadata": {},
   "source": [
    "**TRAINHISTORY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d965f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data - trainHistory.csv\n",
    "df_trainHistory = spark.read.csv(\n",
    "        data_file_trainHistory, \n",
    "        header=True, sep=',', inferSchema=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3753e77",
   "metadata": {},
   "source": [
    "**TRANSACTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93b23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = spark.read.csv(\n",
    "    data_file_transactions, \n",
    "    header=True, sep=',', inferSchema=False\n",
    ").sample(fraction=0.001, seed=42).limit(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593ae82",
   "metadata": {},
   "source": [
    "**KAFKA-CLUSTER-CREATE-TOPIC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ecbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "my_topic = 'shoppers'\n",
    "try:\n",
    "    admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n",
    "    topic = NewTopic(name=my_topic, num_partitions=1, replication_factor=1)\n",
    "    admin.create_topics([topic])\n",
    "    print(f'DEBUG: Topic {my_topic} successfully created.')\n",
    "except Exception as e:\n",
    "    print(f'Error creating topic.\\n{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3142c87",
   "metadata": {},
   "source": [
    "**KAFKA-CLUSTER-INFO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.cluster import ClusterMetadata\n",
    "\n",
    "try:\n",
    "    admin = KafkaAdminClient(bootstrap_servers='localhost:9092')\n",
    "    print(f'List of topics in the cluster: {admin.list_topics()}')\n",
    "    print(f'List of consumer groups known to the cluster: {admin.list_consumer_groups()}')\n",
    "except Exception:\n",
    "    print(f'Error connecting to cluster')\n",
    "\n",
    "clusterMetadata = ClusterMetadata(bootstrap_servers=['localhost:9092'])\n",
    "print(f'All brokers metadata: {clusterMetadata.brokers()}')\n",
    "print(f'Partitions for topic shoppers: {clusterMetadata.partitions_for_topic(\"shoppers\")}')\n",
    "print(f'Topics: {clusterMetadata.topics()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f74b6",
   "metadata": {},
   "source": [
    "**KAFKA-PRODUCER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815aa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, datetime, csv\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "topic = 'shoppers'\n",
    "inputfile = './data/validated_testHistory.csv'  # Use test data for streaming\n",
    "chunksize = 10000\n",
    "sleeptime = 0.6\n",
    "\n",
    "def datetime_converter(dt):\n",
    "    if isinstance(dt, datetime.datetime):\n",
    "        return dt.__str__()\n",
    "\n",
    "def on_send_success(metadata):\n",
    "    print(f'Published to Topic: {metadata.topic}, Partition: {metadata.partition}, Offset: {metadata.offset}')\n",
    "\n",
    "def on_send_error(excp):\n",
    "    print(f'Error: {excp}')\n",
    "\n",
    "kafka_producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    api_version=(3, 9),\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Load and join test data\n",
    "test_df = pd.read_csv(inputfile)\n",
    "offers_df = pd.read_csv('./data/validated_offers.csv')\n",
    "transaction_agg = pd.read_parquet('./data/transaction_features.parquet')\n",
    "df_joined = test_df.merge(offers_df, on='offer', how='left').merge(transaction_agg, on='id', how='left')\n",
    "\n",
    "for _, row in df_joined.iterrows():\n",
    "    json_data = json.dumps(row.to_dict(), default=datetime_converter)\n",
    "    print(f'Sending to Kafka: {json_data}')\n",
    "    try:\n",
    "        kafka_producer.send(topic, value=json_data).add_callback(on_send_success).add_errback(on_send_error)\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "    time.sleep(sleeptime)\n",
    "\n",
    "kafka_producer.flush()\n",
    "kafka_producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b957d8",
   "metadata": {},
   "source": [
    "**KAFKA-CONSUMER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "topic = 'shoppers'\n",
    "kafka_consumer = KafkaConsumer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    api_version=(3, 9),\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=False\n",
    ")\n",
    "\n",
    "kafka_consumer.subscribe([topic])\n",
    "print(f\"Listening on topic: {topic}\")\n",
    "\n",
    "for msg in kafka_consumer:\n",
    "    print(f\"Received: [{msg.topic}:{msg.partition}:{msg.offset}] value={msg.value.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7eaf5",
   "metadata": {},
   "source": [
    "**APP_SPARK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Importing critical functions that deal with data stream\n",
    "from data_streaming import ( spark_initialize, data_stream_spark, \n",
    "            show_status, show_tables, show_sink_table, get_table_dataframe )\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "topic = 'books-amazon'\n",
    "table = 'bookstable'\n",
    "\n",
    "# Showing results of data stream processing\n",
    "def show_spark_results(spark, table):\n",
    "    df = get_table_dataframe(spark, table)\n",
    "    # cols_interest = ['timestamp','Asin','Group','Format','Title','Author','Publisher']\n",
    "    \n",
    "    sleeptime = 0.8\n",
    "    maxiterations = 30\n",
    "    top_authors = 20\n",
    "    top_publishers = 20\n",
    "\n",
    "    # Iterative update\n",
    "    for i in range(maxiterations):\n",
    "        time.sleep(sleeptime)\n",
    "        print(f'Processing...  Iteration {i} with in-between delay of {sleeptime} second(s)')\n",
    "        print(f'Number of records processed so far: {df.count()}.')\n",
    "\n",
    "        #df.select(cols_interest).show(truncate=False)\n",
    "        #df.show(5, truncate=False)\n",
    "        #show_sink_table(spark, table)\n",
    "\n",
    "        print('Aggregated information as it stands (top 20):')\n",
    "        df.groupBy('Author').count().orderBy('count', ascending=False).limit(top_authors).show(truncate=False)\n",
    "        df.groupBy('Publisher').count().orderBy('count', ascending=False).limit(top_publishers).show(truncate=False)\n",
    "        #df.groupBy('Group', 'Format').count().show(truncate=False)\n",
    "        \n",
    "\n",
    "# Execution\n",
    "\n",
    "spark = spark_initialize()\n",
    "query = data_stream_spark(spark, brokers, topic, table)\n",
    "\n",
    "show_status(spark, query)\n",
    "show_tables(spark)\n",
    "show_sink_table(spark, table)\n",
    "show_spark_results(spark, table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300d90d",
   "metadata": {},
   "source": [
    "**APP_STREAMLIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import time\n",
    "\n",
    "# Importing critical functions that deal with data stream (Spark/Kafka side)\n",
    "from data_streaming import ( spark_initialize, data_stream_spark, \n",
    "                show_tables, show_status, get_table_dataframe )\n",
    "\n",
    "# Caching the function that will access the running \n",
    "# Spark/Kafka data query (a DataFrame)\n",
    "@st.cache_resource\n",
    "def get_data():\n",
    "    return get_table_dataframe(st.session_state.spark, st.session_state.table)\n",
    "\n",
    "# Showing results of data stream processing, \n",
    "# as long as there is a SparkSession running\n",
    "def results():\n",
    "\n",
    "    if 'spark' not in st.session_state:\n",
    "        return\n",
    "    \n",
    "    status_text = st.empty()\n",
    "    progress_bar = st.progress(0)\n",
    "    placeholder = st.empty()\n",
    "    sleeptime = 0.8\n",
    "    maxiterations = 30\n",
    "    top_authors = 20\n",
    "    top_publishers = 20\n",
    "\n",
    "    # Iterative update\n",
    "    for i in range(maxiterations):\n",
    "        time.sleep(sleeptime)\n",
    "        # getting data at this point in time\n",
    "        df = get_data()\n",
    "        count = df.count()\n",
    "        status_text.warning(f'Processing...  Iteration {i} with in-between delay of {sleeptime} second(s). Messages/records processed so far: {count}.')\n",
    "        cols1 = ['Author'] \n",
    "        df_author = df.groupBy(cols1).count().orderBy('count', ascending=False).limit(top_authors).toPandas()\n",
    "        cols2 = ['Publisher']\n",
    "        df_publisher = df.groupBy(cols2).count().orderBy('count', ascending=False).limit(top_publishers).toPandas()\n",
    "        print(df_publisher)\n",
    "\n",
    "        with placeholder.container():\n",
    "\n",
    "            # Each chart in one column, so two columns required\n",
    "            fig_col1, fig_col2 = st.columns(2)\n",
    "            with fig_col1:\n",
    "                st.markdown('### Author')\n",
    "                st.markdown(f'**Counting of books by author - Top {top_authors}**')\n",
    "                st.bar_chart(data=df_author, y='count', x=cols1[0], horizontal=True)\n",
    "            with fig_col2:\n",
    "                st.markdown('### Publisher')\n",
    "                st.markdown(f'**Counting of books by publisher - Top {top_publishers}**')\n",
    "                st.bar_chart(data=df_publisher, y='count', x=cols2[0], horizontal=True)\n",
    "\n",
    "            # Show the related dataframes\n",
    "            st.markdown('### Detailed tables view')\n",
    "            st.markdown('**Author**')\n",
    "            st.dataframe(df_author)\n",
    "            st.markdown('**Publisher**')\n",
    "            st.dataframe(df_publisher)\n",
    "    \n",
    "        progress_bar.progress(i)\n",
    "  \n",
    "    progress_bar.empty()\n",
    "    status_text.success(f'Final results are shown after processing {count} messages/records.')\n",
    "\n",
    "# Page to hold results\n",
    "def page_results():\n",
    "    st.empty()\n",
    "    st.header(':one: Data stream processing')\n",
    "    st.subheader('Results')\n",
    "    results()\n",
    "    \n",
    "# Page to hold information about the app\n",
    "def page_about():\n",
    "    st.empty()\n",
    "    st.header(':two: About')\n",
    "    st.subheader('Lab class handout #6')\n",
    "    st.write('Data streaming with Apache Spark and Apache Kafka')\n",
    "    st.badge('Streamlit version', icon='ℹ️', color='blue')\n",
    "    \n",
    "# Entry point\n",
    "def main():\n",
    "    \n",
    "    # Page config\n",
    "    st.set_page_config(\n",
    "        page_title = 'Books data streaming',\n",
    "        initial_sidebar_state = 'expanded',\n",
    "        layout = 'wide'\n",
    "    )\n",
    "    # App title\n",
    "    st.title('Books data streaming')\n",
    "    st.divider()\n",
    "    with st.sidebar:\n",
    "        st.empty()\n",
    "        st.header('Algoritmos para Big Data')\n",
    "\n",
    "    brokers = 'localhost:9092'\n",
    "    topic = 'books-amazon'\n",
    "    table = 'bookstable'\n",
    "\n",
    "    # As code is running everytime the user interacts with, \n",
    "    # we must make sure that the spark side only starts once\n",
    "\n",
    "    if 'spark' not in st.session_state:\n",
    "        spark = spark_initialize()\n",
    "        query = data_stream_spark(spark, brokers, topic, table)\n",
    "        st.session_state.spark = spark\n",
    "        st.session_state.table = table\n",
    "        # just to check in the terminal\n",
    "        show_status(spark, query)\n",
    "        show_tables(spark)\n",
    "        \n",
    "    pages = [ st.Page(page_results, title='Results'),\n",
    "              st.Page(page_about, title='About'),\n",
    "            ]\n",
    "    pg = st.navigation(pages)\n",
    "    pg.run()\n",
    "\n",
    "# Execution\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86329f",
   "metadata": {},
   "source": [
    "**DATA_STREAMING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "def spark_initialize() -> SparkSession:\n",
    "    scala_version = '2.12'\n",
    "    spark_version = '3.3.1'\n",
    "    packages = [\n",
    "        f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "        f'org.apache.spark:spark-token-provider-kafka-0-10_{scala_version}:{spark_version}',\n",
    "        f'org.apache.spark:spark-streaming-kafka-0-10_{scala_version}:{spark_version}',\n",
    "        'org.apache.kafka:kafka-clients:3.3.1',\n",
    "        'org.apache.commons:commons-pool2:2.8.0'\n",
    "    ]\n",
    "    spark = SparkSession.builder\\\n",
    "        .appName('Streaming')\\\n",
    "        .config('spark.jars.packages', ','.join(packages))\\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark\n",
    "\n",
    "def data_stream_spark(spark, brokers, topic, table) -> DataFrame:\n",
    "    df = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", brokers) \\\n",
    "        .option(\"subscribe\", topic) \\\n",
    "        .option(\"includeHeaders\", \"true\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "    spark.sql(f'drop table if exists {table}')\n",
    "    query = df.writeStream \\\n",
    "        .queryName(f'{table}') \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .start()\n",
    "    return query\n",
    "\n",
    "def show_status(spark, query):\n",
    "    print(f'Active: {spark.streams.active[0].isActive}.')\n",
    "    print(f'Status: {query.status}.')\n",
    "\n",
    "def show_tables(spark):\n",
    "    spark.sql(\"show tables\").show(truncate=False)\n",
    "\n",
    "def show_sink_table(spark, table):\n",
    "    spark.sql(f'select * from {table}').show(truncate=False)\n",
    "\n",
    "def get_table_dataframe(spark, table):\n",
    "    df_kafka = spark.sql(f'select CAST(value AS STRING), topic, timestamp from {table}')\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"id\", T.StringType(), True),\n",
    "        T.StructField(\"offer\", T.StringType(), True),\n",
    "        T.StructField(\"category\", T.StringType(), True),\n",
    "        T.StructField(\"quantity\", T.DoubleType(), True),\n",
    "        T.StructField(\"avg_purchase\", T.DoubleType(), True),\n",
    "        T.StructField(\"transaction_count\", T.DoubleType(), True),\n",
    "        T.StructField(\"unique_categories\", T.DoubleType(), True)\n",
    "    ])\n",
    "    df_kafka = df_kafka.withColumn('jsonvalue', F.from_json(F.col('value'), schema)) \\\n",
    "        .select(\n",
    "            F.col('jsonvalue.id').alias('id'),\n",
    "            F.col('jsonvalue.offer').alias('offer'),\n",
    "            F.col('jsonvalue.category').alias('category'),\n",
    "            F.col('jsonvalue.quantity').alias('quantity'),\n",
    "            F.col('jsonvalue.avg_purchase').alias('avg_purchase'),\n",
    "            F.col('jsonvalue.transaction_count').alias('transaction_count'),\n",
    "            F.col('jsonvalue.unique_categories').alias('unique_categories'),\n",
    "            F.col('timestamp')\n",
    "        )\n",
    "    return df_kafka"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
