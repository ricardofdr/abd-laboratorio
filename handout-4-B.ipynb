{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Algoritmos para Big Data\n",
    "\n",
    "**Handout 4 - Machine learning problem - binary classification**\n",
    "\n",
    "**2024/25**\n",
    "\n",
    "This lab class is about binary classification in a discrete space. We will setup a ML processing pipeline to achieve the goals, and the data to be considered relates to the domain of banking industry. Specifically, it is about fraud detection in credit cards transactions\n",
    "\n",
    "This notebook should contain only the implementation of the task B presented in the handout.\n",
    "\n",
    "Hence both handout and notebooks must be considered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task B - ML classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets**\n",
    "\n",
    "In case of clean data is needed, after task A, there are two parquet files available in the data server.\n",
    "\n",
    "The archive files can be can be downloaded from: \n",
    "\n",
    "https://bigdata.iscte-iul.eu/datasets/cards-transactions.zip\n",
    "\n",
    "https://bigdata.iscte-iul.eu/datasets/cards-transactions-small.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder.appName(\"BinaryClassificationB\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and checking data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data_dir =  \n",
    "file_transactions = data_dir + 'cards-transactions-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = spark.read.parquet(file_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_clean - number of rows: 7315741\n",
      "root\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- NumericAmount: float (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Min: integer (nullable = true)\n",
      "\n",
      "+----+----+----+-----+---+-------------------+------------------+--------------------+-------------+----+---------+-------------+----+---+\n",
      "|User|Card|Year|Month|Day|               Time|          Use Chip|       Merchant Name|Merchant City| MCC|Is Fraud?|NumericAmount|Hour|Min|\n",
      "+----+----+----+-----+---+-------------------+------------------+--------------------+-------------+----+---------+-------------+----+---+\n",
      "|   0|   0|2004|    8|  1|2025-03-31 13:09:00| Swipe Transaction| 2027553650310142703|    Mira Loma|5541|       No|        10.02|  13|  9|\n",
      "|   0|   0|2006|   11|  4|2025-03-31 06:42:00| Swipe Transaction|-5475680618560174533|Monterey Park|5942|       No|        38.28|   6| 42|\n",
      "|   0|   0|2007|    9| 30|2025-03-31 13:25:00| Swipe Transaction| 1799189980464955940|     Hesperia|5499|       No|         92.0|  13| 25|\n",
      "|   0|   0|2011|    2|  7|2025-03-31 13:57:00| Swipe Transaction| 3189517333335617109|     La Verne|5311|       No|        60.88|  13| 57|\n",
      "|   0|   0|2012|    4| 19|2025-03-31 14:23:00| Swipe Transaction| 2027553650310142703|    Mira Loma|5541|       No|         3.13|  14| 23|\n",
      "|   0|   0|2018|    1| 17|2025-03-31 13:19:00|  Chip Transaction|-4500542936415012428|     La Verne|5814|       No|        12.55|  13| 19|\n",
      "|   0|   1|2014|   10| 26|2025-03-31 06:02:00|Online Transaction|-7421093378627544099|       ONLINE|5311|       No|       134.64|   6|  2|\n",
      "|   0|   1|2014|   12|  1|2025-03-31 13:50:00| Swipe Transaction| 2027553650310142703|    Mira Loma|5541|       No|         5.22|  13| 50|\n",
      "|   0|   1|2016|    1|  5|2025-03-31 06:16:00|  Chip Transaction| 5817218446178736267|     La Verne|5912|       No|        40.65|   6| 16|\n",
      "|   0|   1|2016|   11| 16|2025-03-31 09:32:00|  Chip Transaction|-3345936507911876459|     La Verne|7538|       No|        75.77|   9| 32|\n",
      "+----+----+----+-----+---+-------------------+------------------+--------------------+-------------+----+---------+-------------+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data\n",
    "print(f'df_clean - number of rows: {df_clean.count()}')\n",
    "df_clean. \n",
    "df_clean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is it really clean?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df_clean - number of rows is {df_clean.count() }; after dropDuplicates() applied would be {df_clean.dropDuplicates().count()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking nulls at each column of df_clean')\n",
    "dict_nulls_clean = {col: df_clean.filter(df_clean[col].isNull()).count() for col in df_clean.columns}\n",
    "dict_nulls_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''df_clean - number of rows after dropna(how='any') would be {df_clean.dropna(how='any').count()}.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "\n",
    "# Checking correlations among some columns - numeric types but no nulls\n",
    "\n",
    "# The columns at stake\n",
    "cols_non_numeric = [field.name for field in df_clean.schema.fields if isinstance(\n",
    "    field.dataType, T.TimestampType) or isinstance(field.dataType, T.StringType)]\n",
    "cols_numeric = [col for col in df_clean.columns if col not in cols_non_numeric]\n",
    "\n",
    "cols_corr = cols_numeric\n",
    "# Correlation needs vectors so we have to convert to vector column first\n",
    "# Then assemble columns to compute\n",
    "vector_col = 'corr_features'\n",
    "assembler = VectorAssembler(inputCols=cols_corr, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df_clean).select(vector_col)\n",
    "# Get correlation matrix - it can be Pearson’s (default) or Spearman’s correlation\n",
    "corr = Correlation.corr(df_vector, vector_col)\n",
    "corr_matrix = corr.collect()[0][0].toArray().tolist()\n",
    "\n",
    "corr.show(truncate=False)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot computed correlations\n",
    "# See colour scales in https://plotly.com/python/builtin-colorscales/\n",
    "print(f'Computed correlations among {cols_corr}:')\n",
    "fig = px.imshow(corr_matrix, title='Correlations',\n",
    "                x = cols_corr, y = cols_corr,\n",
    "                color_continuous_scale='Sunsetdark',  # Sunsetdark, RdBu_r\n",
    "                text_auto=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature enginnering**\n",
    "\n",
    "- Creating new column Fraud to be used as label/target column by the algorithm\n",
    "- Defining features to be used in the creation of the model\n",
    "- Assembling an array with the features to be used by the algorithm, with the help of:\n",
    "\n",
    "    StringIndexer(), OneHotEncoder() and vectorAssembler()\n",
    "\n",
    "    See Chapter 10 of the book \"Learning Spark - Lightning-Fast Data Analytics\" for details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new column Fraud, to be used as label/target:\n",
    "#   1 if value in column Is Fraud? is Yes, 0 otherwise\n",
    "df_clean = ( df_clean\n",
    "            .withColumn(\"Fraud\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: ['Time', 'Use Chip', 'Merchant City', 'Is Fraud?']\n",
      "Numeric columns: ['User', 'Card', 'Year', 'Month', 'Day', 'Merchant Name', 'MCC', 'NumericAmount', 'Hour', 'Min']\n"
     ]
    }
   ],
   "source": [
    "# Recall columns at stake\n",
    "print(f'Non-numeric columns: \n",
    "print(f'Numeric columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features to be used in the creation of the model\n",
    "\n",
    "# First, set which columns not to be used as features. \n",
    "\n",
    "# As a starting point, we are going to exclude only those that \n",
    "# really do not make sense, without any correlation analysis whatsoever.\n",
    "# But that analysis must be done in a next round of model tuning\n",
    "# For the time being we exclude: \n",
    "#   (i) time because aka hour, min and (ii) the target\n",
    "\n",
    "cols_not_features = [  \n",
    "\n",
    "# Then, set columns to be used by StringIndexer() and OneHotEncoder()\n",
    "\n",
    "categorical_cols = [i for i in cols_non_numeric if i not in cols_not_features]\n",
    "non_categorical_cols = [i for i in cols_numeric if i not in cols_not_features]\n",
    "index_output_cols = [x + ' Index' for x in categorical_cols]\n",
    "ohe_output_cols = [x + ' OHE' for x in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features to be used (OHE were categorical):\n",
      " ['Use Chip OHE', 'Merchant City OHE', 'User', 'Card', 'Year', 'Month', 'Day', 'Merchant Name', 'MCC', 'NumericAmount', 'Hour', 'Min']\n"
     ]
    }
   ],
   "source": [
    "# Assembling an array with the features to be used by the algorithm,\n",
    "# with the help of StringIndexer(), OneHotEncoder() and vectorAssembler()\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)\n",
    "\n",
    "# Put all input features into a single vector, by using a transformer\n",
    "assembler_inputs = ohe_output_cols + non_categorical_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "print(f'Input features to be used (OHE were categorical):\\n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.\n",
    "\n",
    "Select and train the model\n",
    "- Train/validation split: creation of two dataframes for training and validation respectively, with a split size of 70/30 (%)\n",
    "- Free memory space of the no longer needed initial dataframe\n",
    "- Set the Linear SVC algorithm as the classifier estimator\n",
    "- Set up a ML pipeline configuration, holding the sequence of the four stages previously set:\n",
    "    1. String indexer\n",
    "    2. OHE encoder\n",
    "    3. Vector assembler\n",
    "    4. ML estimator (SVM)\n",
    "- Create the model by fitting the pipeline to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "# Two dataframes for training and validation respectively, with a split size of 70/30 (%)\n",
    "\n",
    "df_train, df_validation = df_clean.randomSplit([0.7, 0.3], 42)\n",
    "# Caching data ... just the training part as it is accessed many times by the algorithm\n",
    "# But, it might not be a good idea if we are using a local computer and large dataset!\n",
    "# df_train.cache()\n",
    "print(f'There are {df_train.count()} rows in the training set and {df_validation.count()} rows in the validation set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train/validation sets as parquet files \n",
    "# Recall that, because it is a sampling, there is not guarantee of \n",
    "# getting the same data split when using the code in a different computer/time. \n",
    "# And we may want to reproduce or share the experiments.\n",
    "\n",
    "df_train.write.mode('overwrite').parquet(\n",
    "df_validation.write.mode('overwrite').parquet("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we already got the data split, delete df_clean to free memory space\n",
    "del df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC algorithm\n",
    "# default: featuresCol='features', labelCol='label', predictionCol='prediction'\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol='Fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a ML pipeline configuration, holding the sequence of the four stages previously set:\n",
    "# 1. string_indexer\n",
    "# 2. ohe_encoder\n",
    "# 3. vec_assembler (related to assembling features into vector)\n",
    "# 4. lsvc (related to ML estimator)\n",
    "\n",
    "pipeline = Pipeline(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in the pipeline for further use, should it be required\n",
    "pipeline.save('pipeline-LinearSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model by fitting the pipeline to the training data\n",
    "# Notice that the model will be a transformer\n",
    "#\n",
    "# Note: in case there are running problems in your computer, set \n",
    "# a lower number of rows to be used in model training\n",
    "\n",
    "# A\n",
    "# model = pipeline.fit(df_train)\n",
    "# B\n",
    "limit_rows = 100000\n",
    "model = pipeline.fit(df_train.limit(limit_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for further use, should it be required.\n",
    "model.save('model-LinearSVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.\n",
    "Evaluate the model \n",
    "\n",
    "- Make predictions by applying the verification data to the model transformer\n",
    "- With the predictions made:\n",
    "\t- Print out the schema of the resulting DataFrame and show the columns:\n",
    "\t\t features, rawPrediction, prediction, Fraud\n",
    "\t- Compute the evaluation metric *areaUnderROC* using *BinaryClassificationEvaluator*\n",
    "    - Compute the confusion matrix\n",
    "    - Based on the confusion matrix, computed the evaluation matrics:\n",
    "        *accuracy*, *precision*, *recall*, *specifity* and *F1 score*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- NumericAmount: float (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Min: integer (nullable = true)\n",
      " |-- Fraud: integer (nullable = false)\n",
      " |-- Use Chip Index: double (nullable = false)\n",
      " |-- Merchant City Index: double (nullable = false)\n",
      " |-- Use Chip OHE: vector (nullable = true)\n",
      " |-- Merchant City OHE: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions by applying the verification data to the transformer\n",
    "df_predictions = model.\n",
    "\n",
    "# Check its schema\n",
    "df_predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric areaUnderROC = 0.9095048127423266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1817459"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the evaluation metrics \n",
    "# - areaUnderROC using BinaryClassificationEvaluator\n",
    "# - accuracy, precision, recall, and f1Measure, using MultilabelClassificationEvaluator\n",
    "\n",
    "# Using BinaryClassificationEvaluator\n",
    "# Regardless of using default values or not, it is good practice to\n",
    "# explicitly specify them, at the least the important ones\n",
    "\n",
    "# areaUnderROC relates to sensitivity (TP rate) and specificity (FP rate)\n",
    "\n",
    "# Columns of interest: features, rawPrediction, prediction, Fraud\n",
    "df_predictions_eval = df_predictions.select('features', \n",
    "                    'rawPrediction', 'prediction', 'Fraud')\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    \n",
    "area_under_ROC = binary_evaluator.evaluate(\n",
    "\n",
    "# Print out result\n",
    "print(f'Metric areaUnderROC = {area_under_ROC}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting of the kind of predictions made\n",
    "df_confusion_matrix = df_predictions_eval.groupBy(\n",
    "df_confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "tp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Fraud')==1)).first()\n",
    "tn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Fraud')==0)).first()\n",
    "fp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Fraud')==0)).first()\n",
    "fn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Fraud')==1)).first()\n",
    "\n",
    "confmat = {'TP': 0.0, 'TN': 0.0, 'FP': 0.0, 'FN': 0.0}\n",
    "if (tp):\n",
    "    confmat['TP'] = tp['count'] * 1.0\n",
    "if (tn):\n",
    "    confmat['TN'] = tn['count'] * 1.0\n",
    "if (fp):\n",
    "    confmat['FP'] = fp['count'] * 1.0\n",
    "if (fn):\n",
    "    confmat['FN'] = fn['count'] * 1.0\n",
    "\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the confusion matrix, computed the evaluation matrics:\n",
    "#   accuracy, precision, recall, specifity and F1 score\n",
    "\n",
    "# PS: Check divisons by 0.0\n",
    "accuracy = \n",
    "precision = \n",
    "recall = \n",
    "specifity = \n",
    "f1score = \n",
    "\n",
    "print('Evaluation metrics based on the confusion matrix:')\n",
    "print(f' Accuracy = {accuracy}')\n",
    "print(f' Precision = {precision}')\n",
    "print(f' Recall = {recall}')\n",
    "print(f' Specifity = {specifity}')\n",
    "print(f' F1 score = {f1score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
